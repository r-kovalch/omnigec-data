{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langid\n",
    "!pip install --upgrade praw\n",
    "!pip install alphabetic\n",
    "!pip install hvplot\n",
    "!pip install alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import praw\n",
    "from omegaconf import OmegaConf\n",
    "import time\n",
    "from prawcore import TooManyRequests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "parameters = OmegaConf.load(\"../../../parameters/reddit.yaml\")\n",
    "load_dotenv(\"../../../.env\")\n",
    "reddit = praw.Reddit(\n",
    "    username=os.environ.get(\"REDDIT_USERNAME\"),\n",
    "    password=os.environ.get(\"REDDIT_PASSWORD\"),\n",
    "    client_id=os.environ.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.environ.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.environ.get(\"REDDIT_USER_AGENT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = list(parameters.data_collection.subreddits.keys())\n",
    "\n",
    "bots = [\n",
    "    \"AutoModerator\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "n_subreddits = 400\n",
    "\n",
    "def safe_request(func, *args, **kwargs):\n",
    "    \"\"\"Retry a Reddit API request if rate limited.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            wait_time = int(e.response.headers.get('Retry-After', 60 * 5))  # Default to 60 seconds if no header\n",
    "            print(f\"Rate limit hit. Retrying after {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "for language in tqdm(languages, desc=\"Language progress\"):\n",
    "    subreddits = parameters.data_collection.subreddits.get(language)\n",
    "\n",
    "    for subreddit in tqdm(subreddits, desc=\"Subreddits progress\", leave=False):\n",
    "        submissions = safe_request(reddit.subreddit(subreddit).hot, limit=n_subreddits)\n",
    "\n",
    "        for submission in tqdm(submissions, desc=f\"Processing submissions in {subreddit}\", leave=True):\n",
    "            if submission.over_18:\n",
    "                continue\n",
    "\n",
    "            # Collect submission title and selftext\n",
    "            data.append({\n",
    "                'language': language,\n",
    "                'subreddit': subreddit,\n",
    "                'is_selftext': submission.is_self,\n",
    "                'is_submission': True,\n",
    "                'is_comment': False,\n",
    "                'is_reply': False,\n",
    "                'author': str(submission.author),\n",
    "                'title': submission.title,\n",
    "                'text': submission.title\n",
    "            })\n",
    "\n",
    "            # Ensure comments are replaced with their expanded versions\n",
    "            comments = safe_request(getattr, submission, \"comments\")\n",
    "            safe_request(comments.replace_more, limit=None)\n",
    "            comments = filter(lambda c: c.author not in bots, comments.list())\n",
    "\n",
    "            for comment in comments:\n",
    "                data.append({\n",
    "                    'language': language,\n",
    "                    'subreddit': subreddit,\n",
    "                    'is_selftext': False,\n",
    "                    'is_submission': False,\n",
    "                    'is_comment': True,\n",
    "                    'is_reply': False,\n",
    "                    'author': str(comment.author),\n",
    "                    'title': submission.title,\n",
    "                    'text': comment.body\n",
    "                })\n",
    "\n",
    "                # Ensure replies are expanded\n",
    "                replies = safe_request(getattr, comment, \"replies\")\n",
    "                safe_request(replies.replace_more, limit=None)\n",
    "                for reply in replies.list():\n",
    "                    data.append({\n",
    "                        'language': language,\n",
    "                        'subreddit': subreddit,\n",
    "                        'is_selftext': False,\n",
    "                        'is_submission': False,\n",
    "                        'is_comment': False,\n",
    "                        'is_reply': True,\n",
    "                        'author': str(reply.author),\n",
    "                        'title': submission.title,\n",
    "                        'text': reply.body\n",
    "                    })\n",
    "\n",
    "\n",
    "# Convert the collected data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop duplicate rows based on the 'text' column and count the duplicates\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(subset='text')\n",
    "after_dedup = len(df)\n",
    "num_duplicates = before_dedup - after_dedup\n",
    "\n",
    "# Print the number of duplicates removed\n",
    "print(f\"Number of duplicate rows removed: {num_duplicates}\")\n",
    "\n",
    "df.to_csv('../../../datasets/reddit_multigec/raw_reddit_multigec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alphabetical(text):\n",
    "    if isinstance(text, str):\n",
    "        # Regex for all specified languages, including full Ukrainian support\n",
    "        pattern = r\"[a-zA-Záčďéěíňóřšťúůýžäöüßõšžα-ωΑ-ΩþæåāēģīķļņūčšžåäöüÁČĎÉĚÍŇÓŘŠŤÚŮÝŽΑ-Ωа-яґєіїА-ЯҐЄІЇ']+\"\n",
    "        return ' '.join(re.findall(pattern, text, flags=re.IGNORECASE))\n",
    "    return None\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column\n",
    "df['alphabetical_text'] = df['text'].apply(extract_alphabetical)\n",
    "\n",
    "df.loc[:, \"alphabetical_text\"] = df.loc[:, \"text\"].progress_apply(lambda x: extract_alphabetical(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"len_text\"] = df.loc[:, \"text\"].progress_apply(lambda x: len(x))\n",
    "df.loc[:, \"len_alphabetical_text\"] = df.loc[:, \"alphabetical_text\"].progress_apply(lambda x: len(x))\n",
    "df.loc[:, \"alphabetical_ratio\"] = df.progress_apply(lambda x: x[\"len_alphabetical_text\"] / x[\"len_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"language\")[\"alphabetical_ratio\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"language\", \"alphabetical_ratio\"]].hvplot.kde(\n",
    "    by=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"language\", \"alphabetical_ratio\"]].hvplot.box(\n",
    "    by=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    invert=True,\n",
    "    tools=[\"zoom_in\", \"zoom_out\", \"hover\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"language\", \"len_alphabetical_text\"]].hvplot.box(\n",
    "    by=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    invert=True,\n",
    "    tools=[\"zoom_in\", \"zoom_out\", \"hover\"],\n",
    "    title=\"len alphabetical by languages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"language\", \"len_alphabetical_text\"]].hvplot.kde(\n",
    "    by=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    invert=False,\n",
    "    tools=[\"zoom_in\", \"zoom_out\", \"hover\"],\n",
    "    title=\"len alphabetical by languages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"filtering texts with number of alphabetical characters less than 20:\")\n",
    "print(len(df.loc[df.loc[:, \"len_alphabetical_text\"] < 20]))\n",
    "\n",
    "df = df.loc[df.loc[:, \"len_alphabetical_text\"] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"language\", \"len_alphabetical_text\"]].hvplot.box(\n",
    "    by=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    invert=True,\n",
    "    tools=[\"zoom_in\", \"zoom_out\", \"hover\"],\n",
    "    title=\"len alphabetical by languages (after filtering)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_language = df.groupby(\"language\")[\"subreddit\"].count()\n",
    "number_of_text_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_language.reset_index().sort_values(by=\"subreddit\").hvplot.bar(\n",
    "    x=\"language\",\n",
    "    grid=True,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    title=\"Number of samples by language\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"langid_language\"] = df.loc[:, \"alphabetical_text\"].progress_apply(lambda x: langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_map = {\n",
    "    \"cs\": \"czech\",\n",
    "    \"en\": \"english\",\n",
    "    \"et\": \"estonian\",\n",
    "    \"de\": \"german\",\n",
    "    \"el\": \"greek\",\n",
    "    \"is\": \"icelandic\",\n",
    "    \"it\": \"italian\",\n",
    "    \"lv\": \"latvian\",\n",
    "    \"sl\": \"slovene\",\n",
    "    \"sv\": \"swedish\",\n",
    "    \"uk\": \"ukrainian\",\n",
    "}\n",
    "\n",
    "# Translate ISO codes and mark \"other\" for unmatched codes\n",
    "df[\"translated_langid_language\"] = df[\"langid_language\"].map(lang_map).fillna(\"other\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_language_and_langid = df.groupby([\"language\", \"translated_langid_language\"])[\"subreddit\"].count()\n",
    "number_of_text_by_language_and_langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_language_and_langid.reset_index().sort_values(by=\"subreddit\").hvplot.bar(\n",
    "    x=\"language\",\n",
    "    by=\"translated_langid_language\",\n",
    "    grid=True,\n",
    "    width=1400,\n",
    "    height=2000,\n",
    "    title=\"Number of samples by language\",\n",
    "    invert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_langid = df.groupby([\"translated_langid_language\"])[\"subreddit\"].count().rename(\"count\")\n",
    "number_of_text_by_langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_text_by_langid.reset_index().sort_values(by=\"count\").hvplot.bar(\n",
    "    x=\"translated_langid_language\",\n",
    "    grid=True,\n",
    "    width=1400,\n",
    "    height=700,\n",
    "    title=\"Number of samples by langid\",\n",
    "    invert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.loc[:, \"translated_langid_language\"] != \"other\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = df[df[\"translated_langid_language\"] == \"english\"]\n",
    "\n",
    "# Randomly sample 60,000 rows from the English subset\n",
    "sampled_english_df = english_df.sample(n=60000, random_state=42)\n",
    "# Filter rows that are not English\n",
    "non_english_df = df[df[\"translated_langid_language\"] != \"english\"]\n",
    "# Combine the sampled English rows with the non-English rows\n",
    "df = pd.concat([sampled_english_df, non_english_df], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_text_by_langid = df.groupby([\"translated_langid_language\"])[\"subreddit\"].count().rename(\"count\")\n",
    "display(number_of_text_by_langid)\n",
    "\n",
    "number_of_text_by_langid.reset_index().sort_values(by=\"count\").hvplot.bar(\n",
    "    x=\"translated_langid_language\",\n",
    "    grid=True,\n",
    "    width=1400,\n",
    "    height=700,\n",
    "    title=\"Number of samples by langid\",\n",
    "    invert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"translated_langid_language\", \"alphabetical_ratio\"]].hvplot.box(\n",
    "    by=\"translated_langid_language\",\n",
    "    grid=True,\n",
    "    width=900,\n",
    "    height=500,\n",
    "    tools=[\"zoom_in\", \"zoom_out\", \"hover\"],\n",
    "    invert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\n",
    "    \"title\",\n",
    "    \"text\",\n",
    "    \"translated_langid_language\"\n",
    "]].rename(columns={\n",
    "    \"translated_langid_language\": \"language\",\n",
    "}).to_csv(\"../../../datasets/reddit_multigec/pre_moderation_reddit_multigec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
