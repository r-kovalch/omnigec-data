{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from omegaconf import OmegaConf\n",
    "from openai import OpenAI\n",
    "from flatten_dict import flatten\n",
    "from src.utils.output_formatters import try_to_extract_dict_from_json_openai\n",
    "import json\n",
    "\n",
    "from typing import Union\n",
    "from src.prompts.ubertext import (\n",
    "    MultiGrammarErrorCorrectionGenerationPrompt,\n",
    "    GrammarErrorCorrectionAggregationPrompt\n",
    ")\n",
    "from src.utils.openai_batch_utils import submit_openai_batch\n",
    "from src.utils.utils import get_multi_gec_correction_comparison_text\n",
    "from src.utils.metrics import average_edit_distance\n",
    "from src.utils.utils import normalize_spaces, generate_original_corrected_texts\n",
    "from src.utils.comparison_to_html import save_comparison_to_html, save_ansi_to_html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_dotenv(\"../../../.env\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = OmegaConf.load(\"../../../parameters/ubertext.batched.yaml\")\n",
    "output_dir = \"../../../datasets/ubertext_gec/batched.nosync\"\n",
    "silver_file: str = f\"{output_dir}/../pre_post_processing_ubertext_gec.csv\"\n",
    "silver_slim_file: str = f\"{output_dir}/../pre_post_processing_ubertext_gec_slim.csv\"\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPEN_AI_API_KEY\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ubertext_df = pd.read_csv(\"../../../datasets/ubertext_gec/raw_ubertext_gec.csv\", index_col=0)\n",
    "ubertext_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grammar_aggregation_prompt = GrammarErrorCorrectionAggregationPrompt().prompt_template\n",
    "\n",
    "print(grammar_aggregation_prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def prepare_jsonl_batches(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    prompt_template: PromptTemplate,\n",
    "    start_at: int,\n",
    "    end_at: int,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Split the dataset into batches and save as JSONL files for batch processing.\n",
    "    \"\"\"\n",
    "    filename = f\"{output_dir}/ubertext_batched_gecaggregation_input_{start_at}:{end_at}.jsonl\"\n",
    "    multigec_output_file = f\"{output_dir}/ubertext_batched_multigec_output_{start_at}:{end_at}.jsonl\"\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    multigec_outputs: list[str] = open(multigec_output_file, \"r\").read().split(\"\\n\")\n",
    "\n",
    "    for i, multigec_output in zip(\n",
    "        range(start_at, end_at + 1),\n",
    "        multigec_outputs\n",
    "    ):\n",
    "        multigec_output = json.loads(multigec_output)[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        corrections: Union[dict, str] = try_to_extract_dict_from_json_openai(multigec_output)\n",
    "\n",
    "        row = df.loc[i]\n",
    "        sample_id: int = str(i)\n",
    "        sample_input_text: str = row[text_column]\n",
    "        sample_request = {\n",
    "            \"custom_id\": sample_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": parameters.gec_aggregation.model_name,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt_template.format(\n",
    "                            original_text=sample_input_text,\n",
    "                            possible_corrections=corrections,\n",
    "                            num_corrections=parameters.multi_gec.num_corrections\n",
    "                        ),\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": parameters.gec_aggregation.temperature,\n",
    "                \"top_p\": parameters.gec_aggregation.top_p,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save the batch to a JSONL file\n",
    "        with open(filename, \"a\") as f:\n",
    "            f.write(json.dumps(sample_request) + \"\\n\")\n",
    "\n",
    "    print(f\"Input request saved to: {filename}\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "gec_aggregation_batches: str = \"gec_aggregation:\\n\"\n",
    "for _, (s_at, e_at) in parameters.processing.batches.multi_gec.items():\n",
    "    # Prepare batches\n",
    "    ubertext_df_subset = ubertext_df.loc[s_at:e_at]\n",
    "    batch_filename = prepare_jsonl_batches(\n",
    "        df=ubertext_df_subset,\n",
    "        text_column=\"sentence\",\n",
    "        prompt_template=grammar_aggregation_prompt,\n",
    "        start_at=s_at,\n",
    "        end_at=e_at,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    batch_metadata = submit_openai_batch(\n",
    "        client=client,\n",
    "        batch_filename=batch_filename,\n",
    "        start_at=s_at,\n",
    "        end_at=e_at,\n",
    "        description_prefix=\"ubertext-batched-gecaggregation\"\n",
    "    )\n",
    "    gec_aggregation_batches += f\"\\t{batch_metadata.id}: [{s_at}, {e_at}]\\n\"\n",
    "\n",
    "    print(f\"Batch submitted for processing: {batch_metadata.id}\")\n",
    "\n",
    "print(\"\")\n",
    "print(gec_aggregation_batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from src.utils.openai_batch_utils import retrieve_openai_batch\n",
    "for batch_id, (s_at, e_at) in parameters.processing.batches.gec_aggregation.items():\n",
    "    output_file = retrieve_openai_batch(\n",
    "        client=client,\n",
    "        batch_id=batch_id,\n",
    "        output_dir=output_dir,\n",
    "        start_at=s_at,\n",
    "        end_at=e_at,\n",
    "        description_prefix=\"ubertext_batched_gecaggregation\",\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from json import JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level = logging.INFO,format = '[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s')\n",
    "\n",
    "if not os.path.exists(silver_file):\n",
    "    silver_df: pd.DataFrame = ubertext_df\n",
    "    silver_df.to_csv(silver_file, index=False)\n",
    "else:\n",
    "    silver_df = pd.read_csv(silver_file, index_col=\"index\")\n",
    "\n",
    "for _, (s_at, e_at) in tqdm(parameters.processing.batches.gec_aggregation.items(), desc=\"Batches progress\"):\n",
    "    multigec_output_file = f\"{output_dir}/ubertext_batched_multigec_output_{s_at}:{e_at}.jsonl\"\n",
    "    gecaggregation_output_file = f\"{output_dir}/ubertext_batched_gecaggregation_output_{s_at}:{e_at}.jsonl\"\n",
    "    multigec_outputs: list[str] = open(multigec_output_file, \"r\").read().split(\"\\n\")\n",
    "    gecaggregation_outputs: list[str] = open(gecaggregation_output_file, \"r\").read().split(\"\\n\")\n",
    "\n",
    "    for gec_aggregation_output in gecaggregation_outputs:\n",
    "        try:\n",
    "            id = int(json.loads(gec_aggregation_output)[\"custom_id\"])\n",
    "            relative_element = id - s_at\n",
    "            multi_gec_output = multigec_outputs[relative_element]\n",
    "\n",
    "            multi_gec_output: dict[str] = json.loads(multi_gec_output)[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            multi_gec_output: Union[list[dict[str]], str] = try_to_extract_dict_from_json_openai(multi_gec_output)\n",
    "\n",
    "            if isinstance(multi_gec_output, list):\n",
    "                multi_gec_corrections: list[str] = [gec_output[\"correction\"] for gec_output in multi_gec_output]\n",
    "\n",
    "                silver_df.loc[id, \"multi_gec_corrections\"] = str(multi_gec_corrections)\n",
    "            else:\n",
    "                silver_df.loc[id, \"multi_gec_corrections\"] = None\n",
    "                # silver_df.loc[id, \"multi_gec_raw\"] = multi_gec_output\n",
    "\n",
    "            gec_aggregation_output: dict[str] = json.loads(gec_aggregation_output)[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            gec_aggregation_output: Union[dict[str], str] = try_to_extract_dict_from_json_openai(gec_aggregation_output)\n",
    "\n",
    "            if isinstance(gec_aggregation_output, dict):\n",
    "                gec_aggregation_correction: str = gec_aggregation_output[\"correction\"]\n",
    "\n",
    "                silver_df.loc[id, \"gec_aggregation_correction\"] = gec_aggregation_correction\n",
    "            else:\n",
    "                silver_df.loc[id, \"gec_aggregation_correction\"] = None\n",
    "                # silver_df.loc[id, \"gec_aggregation_raw\"] = gec_aggregation_output\n",
    "        except TypeError as e:\n",
    "            logging.error(f\"Got exception: {e} {type(e)}; Skipping: {id} {silver_df.loc[id, 'sentence']}\")\n",
    "        except JSONDecodeError as e:\n",
    "            logging.error(e)\n",
    "\n",
    "silver_df.index.name = \"index\"\n",
    "silver_df.to_csv(silver_file, index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "silver_df = pd.read_csv(silver_file, index_col=\"index\")\n",
    "display(silver_df)\n",
    "silver_slim_df = silver_df[[\"sentence\", \"gec_aggregation_correction\"]].copy()\n",
    "silver_slim_df = silver_slim_df.loc[~silver_slim_df.isna().any(axis=1)]\n",
    "silver_slim_df.to_csv(silver_slim_file, index=True)\n",
    "silver_slim_df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
